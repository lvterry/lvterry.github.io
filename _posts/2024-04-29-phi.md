---
title: 在 Macbook 上运行 Phi-3
date: 2024-04-27 21:44:00 +08:00
categories: essay
layout: post
---

Phi-3-mini 是微软发布的一个小语言模型，有38亿参数，主打一个体积小，能在端侧流畅运行，并且效果不错。

今天试着在自己的 Macbook 上跑了一下。

主要用到了一个开源的工具 [Ollama](https://ollama.com/)，这个工具把在本地运行模型的流程做到了“傻瓜式”。除了 Phi-3 外，它还支持许多主流开源模型，包括 Llama 3, Mistral, Gemma 等。

![Ollama website](https://objectstorageapi.bja.sealos.run/nvkj4xo6-wangyazhou.com/blog/ollama.png)

从官网下载 Ollama 安装包后一路点下一步进行安装。

之后打开终端，运行一个命令 `ollama run phi3` 系统就会自动下载 Phi-3 并安装。这个模型体积只有2.3GB，比我预想的小很多。

安装完成后，终端里会出现一个交互界面，在这里就可以直接开聊了。

![Install complete](https://objectstorageapi.bja.sealos.run/nvkj4xo6-wangyazhou.com/blog/terminal.png)

用起来第一感觉是速度非常快。对简单问题的回答也比较令人满意。

![Q&A in terminal](https://objectstorageapi.bja.sealos.run/nvkj4xo6-wangyazhou.com/blog/terminal2.png)

终端里给出答案无法格式化 markdown 的内容，这可以通过给它套一层 Web UI 来解决。

开源社区也有解决方案，比如 [Open WebUI](https://github.com/open-webui/open-webui) 就可以与 Ollama 无缝集成。根据 Github 上的说明安装好之后（需要预先安装 Docker，之后也是执行一条命令），就可以通过浏览器访问运行在本地的 Web 聊天界面了。

![Open WebUI screen](https://objectstorageapi.bja.sealos.run/nvkj4xo6-wangyazhou.com/blog/open-webui.png)

整体操作下来，感觉现在围绕大模型应用的工具链真的很成熟了，对新手很友好。而且在本地跑模型和直接调用 API 时的感受很不一样，看到它吐出答案的那一刻，很难想象 LLM 这样强大的技术竟然可以在自己的笔记本电脑上运行起来。
